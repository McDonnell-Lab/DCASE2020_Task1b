{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n this v2, don't quadruple the final layer\n",
    "#in this v3, lower the weight decay\n",
    "#in this v5, attempt class balance\n",
    "#v5 worked! In this v7, try increasing the batch size\n",
    "#v8: batch size increase may have been slightly detrimental.\n",
    "    #- go back to batch size 33\n",
    "    #- here experiment with no deltas\n",
    "#v9 - deltas add no value to performance. Here try a substantially smaller model\n",
    "    #-result: hardly lost an accuracy!\n",
    "#v10 put batch size back done and wd back up\n",
    "\n",
    "#this v2 - four time larger model\n",
    "\n",
    "#v3: change kernel for penultimate layer to 3x3 not 1x1\n",
    "\n",
    "#v4: v3 seemed to help slightly\n",
    "# - try lwer weight decay, and load spectrograms from disk\n",
    "\n",
    "#v5: use larger spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a GPU\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mdmcdonn/anaconda3/envs/April2019/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/mdmcdonn/anaconda3/envs/April2019/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/mdmcdonn/anaconda3/envs/April2019/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/mdmcdonn/anaconda3/envs/April2019/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/mdmcdonn/anaconda3/envs/April2019/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/mdmcdonn/anaconda3/envs/April2019/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Librosa version =  0.6.3\n",
      "Pysoundfile version =  0.10.2\n",
      "keras version =  2.2.4-tf\n",
      "tensorflow version =  1.13.1\n"
     ]
    }
   ],
   "source": [
    "#imports \n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import soundfile as sound\n",
    "\n",
    "import tensorflow\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "print(\"Librosa version = \",librosa.__version__)\n",
    "print(\"Pysoundfile version = \",sound.__version__)\n",
    "print(\"keras version = \",tensorflow.keras.__version__)\n",
    "print(\"tensorflow version = \",tensorflow.__version__)\n",
    "\n",
    "sys.path.insert(0, \"../\")\n",
    "from NNets import model_resnet_updated\n",
    "\n",
    "from DCASE_training_functions import LR_WarmRestart, MixupGenerator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BasePath = '../../Data/TAU-urban-acoustic-scenes-2020-3class-development/'\n",
    "TrainFile = BasePath + 'evaluation_setup/fold1_train.csv'\n",
    "ValFile = BasePath + 'evaluation_setup/fold1_evaluate.csv'\n",
    "sr = 48000\n",
    "num_audio_channels = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "SampleDuration = 10\n",
    "\n",
    "#log-mel spectrogram parameters\n",
    "NumFreqBins = 256\n",
    "NumFFTPoints = 4996\n",
    "HopLength = int(NumFFTPoints/4)\n",
    "NumTimeBins = int(np.ceil(SampleDuration*sr/HopLength))\n",
    "\n",
    "#training parameters\n",
    "init_lr = 0.1\n",
    "batch_size = 30 #divisible by 3, due to class balance strategy\n",
    "num_epochs = 510\n",
    "mixup_alpha = 0.4\n",
    "crop_length = 400\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#load filenames and labels\n",
    "dev_train_df = pd.read_csv(TrainFile,sep='\\t', encoding='ASCII')\n",
    "dev_val_df = pd.read_csv(ValFile,sep='\\t', encoding='ASCII')\n",
    "wavpaths_train = dev_train_df['filename'].tolist()\n",
    "wavpaths_val = dev_val_df['filename'].tolist()\n",
    "y_train_labels =  dev_train_df['scene_label'].astype('category').cat.codes.values\n",
    "y_val_labels =  dev_val_df['scene_label'].astype('category').cat.codes.values\n",
    "\n",
    "ClassNames = np.unique(dev_train_df['scene_label'])\n",
    "NumClasses = len(ClassNames)\n",
    "\n",
    "y_train = tensorflow.keras.utils.to_categorical(y_train_labels, NumClasses)\n",
    "y_val = tensorflow.keras.utils.to_categorical(y_val_labels, NumClasses)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['indoor', 'outdoor', 'transportation'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ClassNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "LM_train=np.load('Task1b_LM_train_256_4096.npy')\n",
    "LM_val=np.load('Task1b_LM_val_256_4096.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "LM_train = np.log(LM_train+1e-8)\n",
    "LM_val = np.log(LM_val+1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9185, 256, 469, 2), (4185, 256, 469, 2))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LM_train.shape,LM_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mdmcdonn/anaconda3/envs/April2019/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 256, None, 2) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 128, None, 2) 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 128, None, 2) 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1 (BatchNo (None, 128, None, 2) 8           lambda[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_1 (Batch (None, 128, None, 2) 8           lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 128, None, 25 450         batch_normalization_v1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 128, None, 25 450         batch_normalization_v1_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_2 (Batch (None, 128, None, 25 50          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_3 (Batch (None, 128, None, 25 50          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 128, None, 25 0           batch_normalization_v1_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 128, None, 25 0           batch_normalization_v1_3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 128, None, 25 5625        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 128, None, 25 5625        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_4 (Batch (None, 128, None, 25 50          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_5 (Batch (None, 128, None, 25 50          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 128, None, 25 0           batch_normalization_v1_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 128, None, 25 0           batch_normalization_v1_5[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 128, None, 25 5625        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 128, None, 25 5625        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 128, None, 25 0           conv2d_4[0][0]                   \n",
      "                                                                 conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 128, None, 25 0           conv2d_5[0][0]                   \n",
      "                                                                 conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_6 (Batch (None, 128, None, 25 50          add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_7 (Batch (None, 128, None, 25 50          add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 128, None, 25 0           batch_normalization_v1_6[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 128, None, 25 0           batch_normalization_v1_7[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 128, None, 25 5625        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 128, None, 25 5625        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_8 (Batch (None, 128, None, 25 50          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_9 (Batch (None, 128, None, 25 50          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 128, None, 25 0           batch_normalization_v1_8[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 128, None, 25 0           batch_normalization_v1_9[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 128, None, 25 5625        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 128, None, 25 5625        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 128, None, 25 0           conv2d_8[0][0]                   \n",
      "                                                                 add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 128, None, 25 0           conv2d_9[0][0]                   \n",
      "                                                                 add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_10 (Batc (None, 128, None, 25 50          add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_11 (Batc (None, 128, None, 25 50          add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 128, None, 25 0           batch_normalization_v1_10[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 128, None, 25 0           batch_normalization_v1_11[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 128, None, 50 11250       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 128, None, 50 11250       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_12 (Batc (None, 128, None, 50 100         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d (AveragePooli (None, 128, None, 25 0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_13 (Batc (None, 128, None, 50 100         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 128, None, 25 0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 128, None, 50 0           batch_normalization_v1_12[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 128, None, 25 0           average_pooling2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 128, None, 50 0           batch_normalization_v1_13[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 128, None, 25 0           average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 128, None, 50 22500       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 128, None, 50 0           average_pooling2d[0][0]          \n",
      "                                                                 lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 128, None, 50 22500       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 128, None, 50 0           average_pooling2d_1[0][0]        \n",
      "                                                                 lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 128, None, 50 0           conv2d_12[0][0]                  \n",
      "                                                                 concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 128, None, 50 0           conv2d_13[0][0]                  \n",
      "                                                                 concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_14 (Batc (None, 128, None, 50 100         add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_15 (Batc (None, 128, None, 50 100         add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 128, None, 50 0           batch_normalization_v1_14[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 128, None, 50 0           batch_normalization_v1_15[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 128, None, 50 22500       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 128, None, 50 22500       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_16 (Batc (None, 128, None, 50 100         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_17 (Batc (None, 128, None, 50 100         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 128, None, 50 0           batch_normalization_v1_16[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 128, None, 50 0           batch_normalization_v1_17[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 128, None, 50 22500       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 128, None, 50 22500       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 128, None, 50 0           conv2d_16[0][0]                  \n",
      "                                                                 add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 128, None, 50 0           conv2d_17[0][0]                  \n",
      "                                                                 add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_18 (Batc (None, 128, None, 50 100         add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_19 (Batc (None, 128, None, 50 100         add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 128, None, 50 0           batch_normalization_v1_18[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 128, None, 50 0           batch_normalization_v1_19[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 128, None, 10 45000       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 128, None, 10 45000       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_20 (Batc (None, 128, None, 10 200         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 128, None, 50 0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_21 (Batc (None, 128, None, 10 200         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 128, None, 50 0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 128, None, 10 0           batch_normalization_v1_20[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 128, None, 50 0           average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 128, None, 10 0           batch_normalization_v1_21[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 128, None, 50 0           average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 128, None, 10 90000       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 128, None, 10 0           average_pooling2d_2[0][0]        \n",
      "                                                                 lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 128, None, 10 90000       activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 128, None, 10 0           average_pooling2d_3[0][0]        \n",
      "                                                                 lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 128, None, 10 0           conv2d_20[0][0]                  \n",
      "                                                                 concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 128, None, 10 0           conv2d_21[0][0]                  \n",
      "                                                                 concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_22 (Batc (None, 128, None, 10 200         add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_23 (Batc (None, 128, None, 10 200         add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 128, None, 10 0           batch_normalization_v1_22[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 128, None, 10 0           batch_normalization_v1_23[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 128, None, 10 90000       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 128, None, 10 90000       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_24 (Batc (None, 128, None, 10 200         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_25 (Batc (None, 128, None, 10 200         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 128, None, 10 0           batch_normalization_v1_24[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 128, None, 10 0           batch_normalization_v1_25[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 128, None, 10 90000       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 128, None, 10 90000       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 128, None, 10 0           conv2d_24[0][0]                  \n",
      "                                                                 add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 128, None, 10 0           conv2d_25[0][0]                  \n",
      "                                                                 add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_26 (Batc (None, 128, None, 10 200         add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_27 (Batc (None, 128, None, 10 200         add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 128, None, 10 0           batch_normalization_v1_26[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 128, None, 10 0           batch_normalization_v1_27[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 128, None, 20 180000      activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 128, None, 20 180000      activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_28 (Batc (None, 128, None, 20 400         conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_4 (AveragePoo (None, 128, None, 10 0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_29 (Batc (None, 128, None, 20 400         conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_5 (AveragePoo (None, 128, None, 10 0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 128, None, 20 0           batch_normalization_v1_28[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 128, None, 10 0           average_pooling2d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 128, None, 20 0           batch_normalization_v1_29[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 128, None, 10 0           average_pooling2d_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 128, None, 20 360000      activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 128, None, 20 0           average_pooling2d_4[0][0]        \n",
      "                                                                 lambda_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 128, None, 20 360000      activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 128, None, 20 0           average_pooling2d_5[0][0]        \n",
      "                                                                 lambda_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 128, None, 20 0           conv2d_28[0][0]                  \n",
      "                                                                 concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 128, None, 20 0           conv2d_29[0][0]                  \n",
      "                                                                 concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_30 (Batc (None, 128, None, 20 400         add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_31 (Batc (None, 128, None, 20 400         add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 128, None, 20 0           batch_normalization_v1_30[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 128, None, 20 0           batch_normalization_v1_31[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 128, None, 20 360000      activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 128, None, 20 360000      activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_32 (Batc (None, 128, None, 20 400         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_33 (Batc (None, 128, None, 20 400         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 128, None, 20 0           batch_normalization_v1_32[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 128, None, 20 0           batch_normalization_v1_33[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 128, None, 20 360000      activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 128, None, 20 360000      activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 128, None, 20 0           conv2d_32[0][0]                  \n",
      "                                                                 add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 128, None, 20 0           conv2d_33[0][0]                  \n",
      "                                                                 add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 256, None, 20 0           add_14[0][0]                     \n",
      "                                                                 add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_34 (Batc (None, 256, None, 20 400         concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 256, None, 20 0           batch_normalization_v1_34[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 256, None, 40 720000      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_35 (Batc (None, 256, None, 40 800         conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 256, None, 3) 1200        batch_normalization_v1_35[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_36 (Batc (None, 256, None, 3) 12          conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 3)            0           batch_normalization_v1_36[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 3)            0           global_average_pooling2d[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 4,081,128\n",
      "Trainable params: 4,074,614\n",
      "Non-trainable params: 6,514\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#create and compile the model\n",
    "wd = 5e-4\n",
    "num_filters=25\n",
    "\n",
    "model = model_resnet_updated(NumClasses,\n",
    "                     input_shape =[NumFreqBins,None,num_audio_channels], \n",
    "                     num_filters =num_filters,\n",
    "                     wd=wd,binarise_weights=False)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer =SGD(lr=init_lr,decay=0, momentum=0.9, nesterov=False),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#create data generator\n",
    "TrainDataGen = MixupGenerator(LM_train, \n",
    "                              y_train, \n",
    "                              batch_size=batch_size,\n",
    "                              alpha=mixup_alpha,\n",
    "                              crop_length=crop_length,\n",
    "                              UseBalance=False)\n",
    "\n",
    "steps_per_epoch =TrainDataGen.__len__()\n",
    "lr_scheduler = LR_WarmRestart(nbatch=steps_per_epoch,\n",
    "                              initial_lr=init_lr, min_lr=init_lr*1e-4,\n",
    "                              epochs_restart = [1.0,3.0, 7.0, 15.0, 31.0, 63.0,127.0,255.0]) \n",
    "callbacks = [lr_scheduler]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainDataGen.ClassCounts,2704*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainDataGen.__len__()#,TrainDataGen.reset_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "306*30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,620):\n",
    "    print(i,TrainDataGen.start_ind)\n",
    "    aaa=TrainDataGen.__getitem__(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainDataGen.start_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(TrainDataGen.UseCount,'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(TrainDataGen.UseCount==0),sum(TrainDataGen.UseCount==10),sum(TrainDataGen.UseCount==20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainDataGen.UseCount.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(270,275):\n",
    "    print(i,TrainDataGen.start_ind)\n",
    "    aaa=TrainDataGen.__getitem__(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mdmcdonn/anaconda3/envs/April2019/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "\n",
      " Start of Epoch Learning Rate = 0.100000\n",
      "Epoch 1/510\n",
      "4185/4185 [==============================] - 27s 7ms/sample - loss: 2.7341 - acc: 0.7011\n",
      "\n",
      " End of Epoch Learning Rate = 0.050005\n",
      "306/306 [==============================] - 217s 708ms/step - loss: 3.4662 - acc: 0.7040 - val_loss: 2.7338 - val_acc: 0.7011\n",
      "\n",
      " Start of Epoch Learning Rate = 0.050005\n",
      "Epoch 2/510\n",
      "4185/4185 [==============================] - 26s 6ms/sample - loss: 2.3289 - acc: 0.8459\n",
      "\n",
      " End of Epoch Learning Rate = 0.000010\n",
      "306/306 [==============================] - 211s 689ms/step - loss: 2.5543 - acc: 0.8138 - val_loss: 2.3287 - val_acc: 0.8459\n",
      "\n",
      " Start of Epoch Learning Rate = 0.100000\n",
      "Epoch 3/510\n",
      "4185/4185 [==============================] - 26s 6ms/sample - loss: 1.8065 - acc: 0.6915\n",
      "\n",
      " End of Epoch Learning Rate = 0.085357\n",
      "306/306 [==============================] - 211s 689ms/step - loss: 2.1378 - acc: 0.7704 - val_loss: 1.8067 - val_acc: 0.6915\n",
      "\n",
      " Start of Epoch Learning Rate = 0.085357\n",
      "Epoch 4/510\n",
      "4185/4185 [==============================] - 26s 6ms/sample - loss: 1.3013 - acc: 0.8160\n",
      "\n",
      " End of Epoch Learning Rate = 0.050005\n",
      "306/306 [==============================] - 211s 688ms/step - loss: 1.5056 - acc: 0.8069 - val_loss: 1.3006 - val_acc: 0.8160\n",
      "\n",
      " Start of Epoch Learning Rate = 0.050005\n",
      "Epoch 5/510\n",
      "4185/4185 [==============================] - 26s 6ms/sample - loss: 1.0969 - acc: 0.8425\n",
      "\n",
      " End of Epoch Learning Rate = 0.014653\n",
      "306/306 [==============================] - 210s 685ms/step - loss: 1.2114 - acc: 0.8454 - val_loss: 1.0965 - val_acc: 0.8425\n",
      "\n",
      " Start of Epoch Learning Rate = 0.014653\n",
      "Epoch 6/510\n",
      "4185/4185 [==============================] - 26s 6ms/sample - loss: 0.8912 - acc: 0.9082\n",
      "\n",
      " End of Epoch Learning Rate = 0.000010\n",
      "306/306 [==============================] - 210s 685ms/step - loss: 1.1146 - acc: 0.8703 - val_loss: 0.8907 - val_acc: 0.9082\n",
      "\n",
      " Start of Epoch Learning Rate = 0.100000\n",
      "Epoch 7/510\n",
      "4185/4185 [==============================] - 26s 6ms/sample - loss: 2.5037 - acc: 0.5773\n",
      "\n",
      " End of Epoch Learning Rate = 0.096194\n",
      "306/306 [==============================] - 209s 684ms/step - loss: 1.0793 - acc: 0.8119 - val_loss: 2.4961 - val_acc: 0.5773\n",
      "\n",
      " Start of Epoch Learning Rate = 0.096194\n",
      "Epoch 8/510\n",
      "4185/4185 [==============================] - 26s 6ms/sample - loss: 0.8408 - acc: 0.7529\n",
      "\n",
      " End of Epoch Learning Rate = 0.085357\n",
      "306/306 [==============================] - 210s 686ms/step - loss: 0.8757 - acc: 0.8237 - val_loss: 0.8400 - val_acc: 0.7529\n",
      "\n",
      " Start of Epoch Learning Rate = 0.085357\n",
      "Epoch 9/510\n",
      "4185/4185 [==============================] - 26s 6ms/sample - loss: 0.7256 - acc: 0.7885\n",
      "\n",
      " End of Epoch Learning Rate = 0.069137\n",
      "306/306 [==============================] - 210s 685ms/step - loss: 0.7616 - acc: 0.8327 - val_loss: 0.7237 - val_acc: 0.7885\n",
      "\n",
      " Start of Epoch Learning Rate = 0.069137\n",
      "Epoch 10/510\n",
      "4185/4185 [==============================] - 26s 6ms/sample - loss: 0.6871 - acc: 0.7596\n",
      "\n",
      " End of Epoch Learning Rate = 0.050005\n",
      "306/306 [==============================] - 209s 684ms/step - loss: 0.6880 - acc: 0.8474 - val_loss: 0.6888 - val_acc: 0.7596\n",
      "\n",
      " Start of Epoch Learning Rate = 0.050005\n",
      "Epoch 11/510\n",
      "4185/4185 [==============================] - 26s 6ms/sample - loss: 0.9165 - acc: 0.5974\n",
      "\n",
      " End of Epoch Learning Rate = 0.030873\n",
      "306/306 [==============================] - 209s 684ms/step - loss: 0.6446 - acc: 0.8621 - val_loss: 0.9190 - val_acc: 0.5974\n",
      "\n",
      " Start of Epoch Learning Rate = 0.030873\n",
      "Epoch 12/510\n",
      "4185/4185 [==============================] - 26s 6ms/sample - loss: 0.5156 - acc: 0.8550\n",
      "\n",
      " End of Epoch Learning Rate = 0.014653\n",
      "306/306 [==============================] - 210s 686ms/step - loss: 0.6061 - acc: 0.8740 - val_loss: 0.5150 - val_acc: 0.8550\n",
      "\n",
      " Start of Epoch Learning Rate = 0.014653\n",
      "Epoch 13/510\n",
      "4185/4185 [==============================] - 26s 6ms/sample - loss: 0.3914 - acc: 0.9094\n",
      "\n",
      " End of Epoch Learning Rate = 0.003816\n",
      "306/306 [==============================] - 210s 686ms/step - loss: 0.5674 - acc: 0.8924 - val_loss: 0.3914 - val_acc: 0.9094\n",
      "\n",
      " Start of Epoch Learning Rate = 0.003816\n",
      "Epoch 14/510\n",
      "4185/4185 [==============================] - 26s 6ms/sample - loss: 0.3328 - acc: 0.9281\n",
      "\n",
      " End of Epoch Learning Rate = 0.000010\n",
      "306/306 [==============================] - 208s 681ms/step - loss: 0.5541 - acc: 0.9010 - val_loss: 0.3322 - val_acc: 0.9281\n",
      "\n",
      " Start of Epoch Learning Rate = 0.100000\n",
      "Epoch 15/510\n",
      "4185/4185 [==============================] - 25s 6ms/sample - loss: 0.7958 - acc: 0.7427\n",
      "\n",
      " End of Epoch Learning Rate = 0.099039\n",
      "306/306 [==============================] - 207s 678ms/step - loss: 0.6853 - acc: 0.8261 - val_loss: 0.7933 - val_acc: 0.7427\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099039\n",
      "Epoch 16/510\n",
      "4185/4185 [==============================] - 25s 6ms/sample - loss: 0.4756 - acc: 0.8573\n",
      "\n",
      " End of Epoch Learning Rate = 0.096194\n",
      "306/306 [==============================] - 207s 677ms/step - loss: 0.6579 - acc: 0.8375 - val_loss: 0.4745 - val_acc: 0.8573\n",
      "\n",
      " Start of Epoch Learning Rate = 0.096194\n",
      "Epoch 17/510\n",
      "4185/4185 [==============================] - 26s 6ms/sample - loss: 1.0667 - acc: 0.4996\n",
      "\n",
      " End of Epoch Learning Rate = 0.091574\n",
      "306/306 [==============================] - 208s 679ms/step - loss: 0.6229 - acc: 0.8540 - val_loss: 1.0670 - val_acc: 0.4996\n",
      "\n",
      " Start of Epoch Learning Rate = 0.091574\n",
      "Epoch 18/510\n",
      "4185/4185 [==============================] - 25s 6ms/sample - loss: 3.0349 - acc: 0.4444\n",
      "\n",
      " End of Epoch Learning Rate = 0.085357\n",
      "306/306 [==============================] - 208s 680ms/step - loss: 0.6280 - acc: 0.8473 - val_loss: 3.0288 - val_acc: 0.4444\n",
      "\n",
      " Start of Epoch Learning Rate = 0.085357\n",
      "Epoch 19/510\n",
      "4185/4185 [==============================] - 26s 6ms/sample - loss: 0.6527 - acc: 0.7804\n",
      "\n",
      " End of Epoch Learning Rate = 0.077781\n",
      "306/306 [==============================] - 208s 680ms/step - loss: 0.6143 - acc: 0.8542 - val_loss: 0.6531 - val_acc: 0.7804\n",
      "\n",
      " Start of Epoch Learning Rate = 0.077781\n",
      "Epoch 20/510\n",
      "4185/4185 [==============================] - 26s 6ms/sample - loss: 0.4112 - acc: 0.8817\n",
      "\n",
      " End of Epoch Learning Rate = 0.069137\n",
      "306/306 [==============================] - 208s 679ms/step - loss: 0.5895 - acc: 0.8680 - val_loss: 0.4104 - val_acc: 0.8817\n",
      "\n",
      " Start of Epoch Learning Rate = 0.069137\n",
      "Epoch 21/510\n",
      "4185/4185 [==============================] - 25s 6ms/sample - loss: 0.8746 - acc: 0.7018\n",
      "\n",
      " End of Epoch Learning Rate = 0.059759\n",
      "306/306 [==============================] - 209s 683ms/step - loss: 0.5718 - acc: 0.8751 - val_loss: 0.8717 - val_acc: 0.7018\n",
      "\n",
      " Start of Epoch Learning Rate = 0.059759\n",
      "Epoch 22/510\n",
      "4185/4185 [==============================] - 25s 6ms/sample - loss: 0.5420 - acc: 0.8136\n",
      "\n",
      " End of Epoch Learning Rate = 0.050005\n",
      "306/306 [==============================] - 208s 679ms/step - loss: 0.5639 - acc: 0.8807 - val_loss: 0.5408 - val_acc: 0.8136\n",
      "\n",
      " Start of Epoch Learning Rate = 0.050005\n",
      "Epoch 23/510\n",
      "4185/4185 [==============================] - 26s 6ms/sample - loss: 0.6739 - acc: 0.7293\n",
      "\n",
      " End of Epoch Learning Rate = 0.040251\n",
      "306/306 [==============================] - 208s 679ms/step - loss: 0.5498 - acc: 0.8828 - val_loss: 0.6718 - val_acc: 0.7293\n",
      "\n",
      " Start of Epoch Learning Rate = 0.040251\n",
      "Epoch 24/510\n",
      "4185/4185 [==============================] - 25s 6ms/sample - loss: 0.3674 - acc: 0.8970\n",
      "\n",
      " End of Epoch Learning Rate = 0.030873\n",
      "306/306 [==============================] - 208s 678ms/step - loss: 0.5294 - acc: 0.8929 - val_loss: 0.3668 - val_acc: 0.8970\n",
      "\n",
      " Start of Epoch Learning Rate = 0.030873\n",
      "Epoch 25/510\n",
      "4185/4185 [==============================] - 26s 6ms/sample - loss: 0.6014 - acc: 0.7338\n",
      "\n",
      " End of Epoch Learning Rate = 0.022229\n",
      "306/306 [==============================] - 209s 681ms/step - loss: 0.5055 - acc: 0.8995 - val_loss: 0.6028 - val_acc: 0.7338\n",
      "\n",
      " Start of Epoch Learning Rate = 0.022229\n",
      "Epoch 26/510\n",
      "4185/4185 [==============================] - 26s 6ms/sample - loss: 0.3572 - acc: 0.8820\n",
      "\n",
      " End of Epoch Learning Rate = 0.014653\n",
      "306/306 [==============================] - 208s 679ms/step - loss: 0.5015 - acc: 0.9074 - val_loss: 0.3568 - val_acc: 0.8820\n",
      "\n",
      " Start of Epoch Learning Rate = 0.014653\n",
      "Epoch 27/510\n",
      "4185/4185 [==============================] - 26s 6ms/sample - loss: 0.2865 - acc: 0.9360\n",
      "\n",
      " End of Epoch Learning Rate = 0.008436\n",
      "306/306 [==============================] - 208s 678ms/step - loss: 0.4887 - acc: 0.9075 - val_loss: 0.2860 - val_acc: 0.9360\n",
      "\n",
      " Start of Epoch Learning Rate = 0.008436\n",
      "Epoch 28/510\n",
      "4185/4185 [==============================] - 26s 6ms/sample - loss: 0.2597 - acc: 0.9262\n",
      "\n",
      " End of Epoch Learning Rate = 0.003816\n",
      "306/306 [==============================] - 209s 682ms/step - loss: 0.4743 - acc: 0.9175 - val_loss: 0.2591 - val_acc: 0.9262\n",
      "\n",
      " Start of Epoch Learning Rate = 0.003816\n",
      "Epoch 29/510\n",
      "4185/4185 [==============================] - 26s 6ms/sample - loss: 0.2259 - acc: 0.9448\n",
      "\n",
      " End of Epoch Learning Rate = 0.000971\n",
      "306/306 [==============================] - 210s 685ms/step - loss: 0.4605 - acc: 0.9222 - val_loss: 0.2255 - val_acc: 0.9448\n",
      "\n",
      " Start of Epoch Learning Rate = 0.000971\n",
      "Epoch 30/510\n",
      "4185/4185 [==============================] - 26s 6ms/sample - loss: 0.2220 - acc: 0.9453\n",
      "\n",
      " End of Epoch Learning Rate = 0.000010\n",
      "306/306 [==============================] - 210s 687ms/step - loss: 0.4643 - acc: 0.9197 - val_loss: 0.2215 - val_acc: 0.9453\n",
      "\n",
      " Start of Epoch Learning Rate = 0.100000\n",
      "Epoch 31/510\n",
      "4185/4185 [==============================] - 26s 6ms/sample - loss: 0.7990 - acc: 0.6509\n",
      "\n",
      " End of Epoch Learning Rate = 0.099759\n",
      "306/306 [==============================] - 210s 687ms/step - loss: 0.6848 - acc: 0.8084 - val_loss: 0.7994 - val_acc: 0.6509\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099759\n",
      "Epoch 32/510\n",
      "4185/4185 [==============================] - 26s 6ms/sample - loss: 1.1810 - acc: 0.5008\n",
      "\n",
      " End of Epoch Learning Rate = 0.099039\n",
      "306/306 [==============================] - 212s 692ms/step - loss: 0.6453 - acc: 0.8422 - val_loss: 1.1824 - val_acc: 0.5008\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099039\n",
      "Epoch 33/510\n",
      "4185/4185 [==============================] - 26s 6ms/sample - loss: 0.5827 - acc: 0.7888\n",
      "\n",
      " End of Epoch Learning Rate = 0.097847\n",
      "306/306 [==============================] - 209s 684ms/step - loss: 0.6400 - acc: 0.8375 - val_loss: 0.5815 - val_acc: 0.7888\n",
      "\n",
      " Start of Epoch Learning Rate = 0.097847\n",
      "Epoch 34/510\n",
      "4185/4185 [==============================] - 26s 6ms/sample - loss: 0.9607 - acc: 0.5761\n",
      "\n",
      " End of Epoch Learning Rate = 0.096194\n",
      "306/306 [==============================] - 209s 684ms/step - loss: 0.6255 - acc: 0.8527 - val_loss: 0.9632 - val_acc: 0.5761\n",
      "\n",
      " Start of Epoch Learning Rate = 0.096194\n",
      "Epoch 35/510\n",
      "4185/4185 [==============================] - 26s 6ms/sample - loss: 1.1834 - acc: 0.5078\n",
      "\n",
      " End of Epoch Learning Rate = 0.094097\n",
      "306/306 [==============================] - 210s 685ms/step - loss: 0.6228 - acc: 0.8546 - val_loss: 1.1873 - val_acc: 0.5078\n",
      "\n",
      " Start of Epoch Learning Rate = 0.094097\n",
      "Epoch 36/510\n",
      "4185/4185 [==============================] - 26s 6ms/sample - loss: 1.0485 - acc: 0.6129\n",
      "\n",
      " End of Epoch Learning Rate = 0.091574\n",
      "306/306 [==============================] - 211s 691ms/step - loss: 0.6093 - acc: 0.8536 - val_loss: 1.0507 - val_acc: 0.6129\n",
      "\n",
      " Start of Epoch Learning Rate = 0.091574\n",
      "Epoch 37/510\n",
      "4185/4185 [==============================] - 26s 6ms/sample - loss: 0.8669 - acc: 0.6832\n",
      "\n",
      " End of Epoch Learning Rate = 0.088652\n",
      "306/306 [==============================] - 210s 685ms/step - loss: 0.6044 - acc: 0.8639 - val_loss: 0.8646 - val_acc: 0.6832\n",
      "\n",
      " Start of Epoch Learning Rate = 0.088652\n",
      "Epoch 38/510\n",
      "4185/4185 [==============================] - 26s 6ms/sample - loss: 1.2926 - acc: 0.4994\n",
      "\n",
      " End of Epoch Learning Rate = 0.085357\n",
      "306/306 [==============================] - 209s 684ms/step - loss: 0.6002 - acc: 0.8585 - val_loss: 1.2959 - val_acc: 0.4994\n",
      "\n",
      " Start of Epoch Learning Rate = 0.085357\n",
      "Epoch 39/510\n",
      "4185/4185 [==============================] - 26s 6ms/sample - loss: 1.3668 - acc: 0.6645\n",
      "\n",
      " End of Epoch Learning Rate = 0.081721\n",
      "306/306 [==============================] - 209s 684ms/step - loss: 0.5977 - acc: 0.8570 - val_loss: 1.3626 - val_acc: 0.6645\n",
      "\n",
      " Start of Epoch Learning Rate = 0.081721\n",
      "Epoch 40/510\n",
      "4185/4185 [==============================] - 26s 6ms/sample - loss: 0.5185 - acc: 0.8237\n",
      "\n",
      " End of Epoch Learning Rate = 0.077781\n",
      "306/306 [==============================] - 210s 685ms/step - loss: 0.6003 - acc: 0.8633 - val_loss: 0.5175 - val_acc: 0.8237\n",
      "\n",
      " Start of Epoch Learning Rate = 0.077781\n",
      "Epoch 41/510\n",
      "4185/4185 [==============================] - 26s 6ms/sample - loss: 0.6193 - acc: 0.7281\n",
      "\n",
      " End of Epoch Learning Rate = 0.073572\n",
      "306/306 [==============================] - 209s 684ms/step - loss: 0.5856 - acc: 0.8685 - val_loss: 0.6176 - val_acc: 0.7281\n",
      "\n",
      " Start of Epoch Learning Rate = 0.073572\n",
      "Epoch 42/510\n",
      "4185/4185 [==============================] - 26s 6ms/sample - loss: 0.8010 - acc: 0.6681\n",
      "\n",
      " End of Epoch Learning Rate = 0.069137\n",
      "306/306 [==============================] - 209s 684ms/step - loss: 0.5824 - acc: 0.8671 - val_loss: 0.8002 - val_acc: 0.6681\n",
      "\n",
      " Start of Epoch Learning Rate = 0.069137\n",
      "Epoch 43/510\n",
      "4185/4185 [==============================] - 26s 6ms/sample - loss: 0.4183 - acc: 0.8499\n",
      "\n",
      " End of Epoch Learning Rate = 0.064518\n",
      "306/306 [==============================] - 211s 690ms/step - loss: 0.5709 - acc: 0.8686 - val_loss: 0.4174 - val_acc: 0.8499\n",
      "\n",
      " Start of Epoch Learning Rate = 0.064518\n",
      "Epoch 44/510\n",
      "4185/4185 [==============================] - 26s 6ms/sample - loss: 0.3131 - acc: 0.9266\n",
      "\n",
      " End of Epoch Learning Rate = 0.059759\n",
      "306/306 [==============================] - 209s 685ms/step - loss: 0.5604 - acc: 0.8814 - val_loss: 0.3125 - val_acc: 0.9266\n",
      "\n",
      " Start of Epoch Learning Rate = 0.059759\n",
      "Epoch 45/510\n",
      "4185/4185 [==============================] - 26s 6ms/sample - loss: 0.3913 - acc: 0.8753\n",
      "\n",
      " End of Epoch Learning Rate = 0.054905\n",
      "306/306 [==============================] - 211s 691ms/step - loss: 0.5531 - acc: 0.8767 - val_loss: 0.3907 - val_acc: 0.8753\n",
      "\n",
      " Start of Epoch Learning Rate = 0.054905\n",
      "Epoch 46/510\n",
      "4185/4185 [==============================] - 26s 6ms/sample - loss: 0.4554 - acc: 0.8332\n",
      "\n",
      " End of Epoch Learning Rate = 0.050005\n",
      "306/306 [==============================] - 210s 686ms/step - loss: 0.5591 - acc: 0.8783 - val_loss: 0.4543 - val_acc: 0.8332\n",
      "\n",
      " Start of Epoch Learning Rate = 0.050005\n",
      "Epoch 47/510\n",
      "4185/4185 [==============================] - 26s 6ms/sample - loss: 0.5370 - acc: 0.7885\n",
      "\n",
      " End of Epoch Learning Rate = 0.045105\n",
      "306/306 [==============================] - 210s 686ms/step - loss: 0.5488 - acc: 0.8843 - val_loss: 0.5365 - val_acc: 0.7885\n",
      "\n",
      " Start of Epoch Learning Rate = 0.045105\n",
      "Epoch 48/510\n",
      "4185/4185 [==============================] - 26s 6ms/sample - loss: 0.5186 - acc: 0.7969\n",
      "\n",
      " End of Epoch Learning Rate = 0.040251\n",
      "306/306 [==============================] - 210s 686ms/step - loss: 0.5234 - acc: 0.8964 - val_loss: 0.5185 - val_acc: 0.7969\n",
      "\n",
      " Start of Epoch Learning Rate = 0.040251\n",
      "Epoch 49/510\n",
      "4185/4185 [==============================] - 26s 6ms/sample - loss: 0.6113 - acc: 0.7699\n",
      "\n",
      " End of Epoch Learning Rate = 0.035492\n",
      "306/306 [==============================] - 211s 691ms/step - loss: 0.5292 - acc: 0.8912 - val_loss: 0.6103 - val_acc: 0.7699\n",
      "\n",
      " Start of Epoch Learning Rate = 0.035492\n",
      "Epoch 50/510\n",
      "4185/4185 [==============================] - 26s 6ms/sample - loss: 0.9649 - acc: 0.5137\n",
      "\n",
      " End of Epoch Learning Rate = 0.030873\n",
      "306/306 [==============================] - 210s 685ms/step - loss: 0.5122 - acc: 0.9017 - val_loss: 0.9673 - val_acc: 0.5137\n",
      "\n",
      " Start of Epoch Learning Rate = 0.030873\n",
      "Epoch 51/510\n",
      "4185/4185 [==============================] - 26s 6ms/sample - loss: 0.7197 - acc: 0.6746\n",
      "\n",
      " End of Epoch Learning Rate = 0.026438\n",
      "306/306 [==============================] - 210s 685ms/step - loss: 0.5086 - acc: 0.8988 - val_loss: 0.7185 - val_acc: 0.6746\n",
      "\n",
      " Start of Epoch Learning Rate = 0.026438\n",
      "Epoch 52/510\n",
      " 81/306 [======>.......................] - ETA: 2:15 - loss: 0.5047 - acc: 0.9008"
     ]
    }
   ],
   "source": [
    "#train the model\n",
    "\n",
    "history = model.fit_generator(TrainDataGen,\n",
    "                              validation_data=(LM_val, y_val),\n",
    "                              epochs=num_epochs, \n",
    "                              verbose=1, \n",
    "                              workers=1,\n",
    "                              max_queue_size = 100,\n",
    "                              callbacks=callbacks,\n",
    "                              steps_per_epoch=steps_per_epoch\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.plot(history.history['acc'])\n",
    "plt.ylim([0.6,0.98])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,4))\n",
    "plt.plot(np.arange(0,len(lr_scheduler.lr_used))/np.ceil(LM_train.shape[0]/batch_size),lr_scheduler.lr_used,'-')\n",
    "plt.xlabel('epoch number')\n",
    "plt.ylabel('learning rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history['val_acc'][13],history.history['val_acc'][29],history.history['val_acc'][61],history.history['val_acc'][125],history.history['val_acc'][253],history.history['val_acc'][509]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(history.history['val_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['val_acc'][450::])\n",
    "plt.plot(history.history['acc'][450::])\n",
    "\n",
    "plt.plot(history.history['val_acc'][0:15])\n",
    "plt.plot(history.history['acc'][0:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import DCASE_plots\n",
    "#importlib.reload(DCASE_plots)\n",
    "from DCASE_plots import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(LM_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#too many predictions for indoor and outdoor\n",
    "PC_correct =sum( np.argmax(y_val,axis=-1)==np.argmax([0.75,1,0.7]*y_pred,axis=-1))/y_pred.shape[0]\n",
    "PC_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(np.argmax(y_val,axis=-1), np.argmax(y_pred,axis=-1), dev_train_df['scene_label'].unique().tolist(),\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(np.argmax(y_val,axis=-1), np.argmax([0.5,1,0.3]*y_pred,axis=-1), dev_train_df['scene_label'].unique().tolist(),\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "weightings = [1,1,1] #justification for transport to bbe higher is each example was used less in training\n",
    "\n",
    "y_pred_val=np.argmax(weightings*y_pred,axis=-1)\n",
    "y_val_labels=np.argmax(y_val,axis=-1)\n",
    "Overall_accuracy = np.sum(y_pred_val==y_val_labels)/LM_val.shape[0]\n",
    "print(\"overall accuracy: \", Overall_accuracy)\n",
    "\n",
    "\n",
    "conf_matrix = confusion_matrix(y_val_labels,y_pred_val)\n",
    "conf_mat_norm_recall = conf_matrix.astype('float32')/conf_matrix.sum(axis=1)[:,np.newaxis]\n",
    "conf_mat_norm_precision = conf_matrix.astype('float32')/conf_matrix.sum(axis=0)[:,np.newaxis]\n",
    "recall_by_class = np.diagonal(conf_mat_norm_recall)\n",
    "precision_by_class = np.diagonal(conf_mat_norm_precision)\n",
    "mean_recall = np.mean(recall_by_class)\n",
    "mean_precision = np.mean(precision_by_class)\n",
    "\n",
    "print(\"per-class accuracy (recall): \",recall_by_class)\n",
    "print(\"per-class precision: \",precision_by_class)\n",
    "print(\"mean per-class recall: \",mean_recall)\n",
    "print(\"mean per-class precision: \",mean_precision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train = np.argmax(y_train,axis=-1)\n",
    "labels_val = np.argmax(y_val,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[np.sum(labels_train==0),np.sum(labels_train==1),np.sum(labels_train==2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[np.sum(labels_val==0),np.sum(labels_val==1),np.sum(labels_val==2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2704/3757"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3750/2700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(labels_val)*.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
